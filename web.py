# è®¾ç½® FFMPEG_PATH ç¯å¢ƒå˜é‡
import os
# ffmpeg_path = os.path.join(os.getcwd(), 'sensvoice', 'ffmpeg', 'bin')
# os.environ['FFMPEG_PATH'] = ffmpeg_path

# # å°† FFMPEG_PATH ä¸´æ—¶æ·»åŠ åˆ° PATH ç¯å¢ƒå˜é‡ä¸­
# os.environ['PATH'] += os.pathsep + ffmpeg_path

import librosa
import base64
import io
import numpy as np
import torch
import torchaudio
from funasr import AutoModel
import sounddevice as sd
import numpy as np
import soundfile as sf
import os
import uvicorn
from fastapi import FastAPI,Request

class SenseVoiceModel:
    def __init__(self):
        self.model = AutoModel(
            model="iic/SenseVoiceSmall",
            vad_model="iic/speech_fsmn_vad_zh-cn-16k-common-pytorch",
            vad_kwargs={"max_single_segment_time": 30000},
            trust_remote_code=True,
        )

        self.emo_dict = {
            "<|HAPPY|>": "ğŸ˜Š",
            "<|SAD|>": "ğŸ˜”",
            "<|ANGRY|>": "ğŸ˜¡",
            "<|NEUTRAL|>": "",
            "<|FEARFUL|>": "ğŸ˜°",
            "<|DISGUSTED|>": "ğŸ¤¢",
            "<|SURPRISED|>": "ğŸ˜®",
        }

        self.event_dict = {
            "<|BGM|>": "ğŸ¼",
            "<|Speech|>": "",
            "<|Applause|>": "ğŸ‘",
            "<|Laughter|>": "ğŸ˜€",
            "<|Cry|>": "ğŸ˜­",
            "<|Sneeze|>": "ğŸ¤§",
            "<|Breath|>": "",
            "<|Cough|>": "ğŸ¤§",
        }

        self.emoji_dict = {
            "<|nospeech|><|Event_UNK|>": "â“",
            "<|zh|>": "",
            "<|en|>": "",
            "<|yue|>": "",
            "<|ja|>": "",
            "<|ko|>": "",
            "<|nospeech|>": "",
            "<|HAPPY|>": "ğŸ˜Š",
            "<|SAD|>": "ğŸ˜”",
            "<|ANGRY|>": "ğŸ˜¡",
            "<|NEUTRAL|>": "",
            "<|BGM|>": "ğŸ¼",
            "<|Speech|>": "",
            "<|Applause|>": "ğŸ‘",
            "<|Laughter|>": "ğŸ˜€",
            "<|FEARFUL|>": "ğŸ˜°",
            "<|DISGUSTED|>": "ğŸ¤¢",
            "<|SURPRISED|>": "ğŸ˜®",
            "<|Cry|>": "ğŸ˜­",
            "<|EMO_UNKNOWN|>": "",
            "<|Sneeze|>": "ğŸ¤§",
            "<|Breath|>": "",
            "<|Cough|>": "ğŸ˜·",
            "<|Sing|>": "",
            "<|Speech_Noise|>": "",
            "<|withitn|>": "",
            "<|woitn|>": "",
            "<|GBG|>": "",
            "<|Event_UNK|>": "",
        }

        self.lang_dict = {
            "<|zh|>": "<|lang|>",
            "<|en|>": "<|lang|>",
            "<|yue|>": "<|lang|>",
            "<|ja|>": "<|lang|>",
            "<|ko|>": "<|lang|>",
            "<|nospeech|>": "<|lang|>",
        }

        self.emo_set = {"ğŸ˜Š", "ğŸ˜”", "ğŸ˜¡", "ğŸ˜°", "ğŸ¤¢", "ğŸ˜®"}
        self.event_set = {"ğŸ¼", "ğŸ‘", "ğŸ˜€", "ğŸ˜­", "ğŸ¤§", "ğŸ˜·"}

    def format_str(self, s):
        for sptk in self.emoji_dict:
            s = s.replace(sptk, self.emoji_dict[sptk])
        return s

    def format_str_v2(self, s):
        sptk_dict = {}
        for sptk in self.emoji_dict:
            sptk_dict[sptk] = s.count(sptk)
            s = s.replace(sptk, "")
        emo = "<|NEUTRAL|>"
        for e in self.emo_dict:
            if sptk_dict[e] > sptk_dict[emo]:
                emo = e
        for e in self.event_dict:
            if sptk_dict[e] > 0:
                s = self.event_dict[e] + s
        s = s + self.emo_dict[emo]

        for emoji in self.emo_set.union(self.event_set):
            s = s.replace(" " + emoji, emoji)
            s = s.replace(emoji + " ", emoji)
        return s.strip()

    def format_str_v3(self, s):
        def get_emo(s):
            return s[-1] if s[-1] in self.emo_set else None

        def get_event(s):
            return s[0] if s[0] in self.event_set else None

        s = s.replace("<|nospeech|><|Event_UNK|>", "â“")
        for lang in self.lang_dict:
            s = s.replace(lang, "<|lang|>")
        s_list = [self.format_str_v2(s_i).strip(" ") for s_i in s.split("<|lang|>")]
        new_s = " " + s_list[0]
        cur_ent_event = get_event(new_s)
        for i in range(1, len(s_list)):
            if len(s_list[i]) == 0:
                continue
            if get_event(s_list[i]) == cur_ent_event and get_event(s_list[i]) != None:
                s_list[i] = s_list[i][1:]
            cur_ent_event = get_event(s_list[i])
            if get_emo(s_list[i]) != None and get_emo(s_list[i]) == get_emo(new_s):
                new_s = new_s[:-1]
            new_s += s_list[i].strip().lstrip()
        new_s = new_s.replace("The.", " ")
        return new_s.strip()

    def model_inference(self, input_wav, language, fs=16000):
        language_abbr = {"auto": "auto", "zh": "zh", "en": "en", "yue": "yue", "ja": "ja", "ko": "ko", "nospeech": "nospeech"}
        language = "auto" if len(language) < 1 else language
        selected_language = language_abbr[language]

        if isinstance(input_wav, tuple):
            fs, input_wav = input_wav
            input_wav = input_wav.astype(np.float32) / np.iinfo(np.int16).max
            if len(input_wav.shape) > 1:
                input_wav = input_wav.mean(-1)
            if fs != 16000:
                resampler = torchaudio.transforms.Resample(fs, 16000)
                input_wav_t = torch.from_numpy(input_wav).to(torch.float32)
                input_wav = resampler(input_wav_t[None, :])[0, :].numpy()

        merge_vad = True
        print(f"language: {language}, merge_vad: {merge_vad}")
        text = self.model.generate(input=input_wav, cache={}, language=language, use_itn=True, batch_size_s=0, merge_vad=merge_vad)
        text = text[0]["text"]
        text = self.format_str_v3(text)
        print(text)
        return text

# def record_audio_manually(output_folder, filename="audio.wav", sample_rate=44100):
#     """
#     æ‰‹åŠ¨æ§åˆ¶å½•åˆ¶çš„å¼€å§‹å’Œç»“æŸï¼Œå¹¶ä¿å­˜éŸ³é¢‘æ•°æ®åˆ°æŒ‡å®šæ–‡ä»¶å¤¹ã€‚

#     å‚æ•°:
#     output_folder (str): ä¿å­˜éŸ³é¢‘æ–‡ä»¶çš„æ–‡ä»¶å¤¹è·¯å¾„ã€‚
#     filename (str): ä¿å­˜éŸ³é¢‘æ–‡ä»¶çš„æ–‡ä»¶åï¼ˆé»˜è®¤"recorded_audio.wav"ï¼‰ã€‚
#     sample_rate (int): éŸ³é¢‘é‡‡æ ·ç‡ï¼ˆé»˜è®¤44100 Hzï¼‰ã€‚

#     è¿”å›:
#     numpy.ndarray: å½•åˆ¶çš„éŸ³é¢‘æ•°æ®ã€‚
#     """
#     print("æŒ‰ä¸‹ Enter é”®å¼€å§‹å½•åˆ¶ï¼Œå†æ¬¡æŒ‰ä¸‹ Enter é”®ç»“æŸå½•åˆ¶...")
#     input()  # ç­‰å¾…ç”¨æˆ·æŒ‰ä¸‹ Enter é”®å¼€å§‹å½•åˆ¶

#     audio_data = []
#     def callback(indata, frames, time, status):
#         """è¿™æ˜¯å½•éŸ³çš„å›è°ƒå‡½æ•°"""
#         if status:
#             print(status)
#         audio_data.append(indata.copy())

#     stream = sd.InputStream(samplerate=sample_rate, channels=2, callback=callback)
#     with stream:
#         input()  # ç­‰å¾…ç”¨æˆ·æŒ‰ä¸‹ Enter é”®ç»“æŸå½•åˆ¶

#     audio_data = np.concatenate(audio_data, axis=0)

#     # ç¡®ä¿è¾“å‡ºæ–‡ä»¶å¤¹å­˜åœ¨
#     if not os.path.exists(output_folder):
#         os.makedirs(output_folder)

#     # ä¿å­˜éŸ³é¢‘æ•°æ®åˆ°æŒ‡å®šæ–‡ä»¶
#     output_path = os.path.join(output_folder, filename)
#     sf.write(output_path, audio_data, sample_rate)
#     print(f"éŸ³é¢‘å·²ä¿å­˜åˆ° {output_path}")



# Example usage
# model = SenseVoiceModel()
# input_wav="sensvoice/recorded_audios/audio.wav"
# language = "auto"
# result = model.model_inference(input_wav, language)

model = None
model = SenseVoiceModel()
app = FastAPI()

# @app.post("/open/")
# async def init_sense_voice_model():
#     global model

#     return {"message": "SenseVoiceModel initialized"}

@app.post("/run/")
async def run_model_inference():
    # å›ºå®šçš„è¾“å…¥å‚æ•°
    input_wav = "recorded_audios/audio1.wav"
    language = "auto"
    
    # æ‰§è¡Œæ¨¡å‹æ¨ç†
    result = model.model_inference(input_wav, language)
    
    # è¿™é‡Œæ²¡æœ‰è¿”å›å€¼ï¼Œä½†å¦‚æœä½ æƒ³è®°å½•æˆ–å¤„ç†ç»“æœï¼Œå¯ä»¥åœ¨è¿™é‡Œè¿›è¡Œ
    # ä¾‹å¦‚ï¼Œæ‰“å°ç»“æœæˆ–å°†å…¶å­˜å‚¨åœ¨æ—¥å¿—ä¸­
    # print(result)  # å‡è®¾resultæ˜¯å¯ä»¥ç›´æ¥æ‰“å°çš„ç±»å‹

    # è¿”å›ä¸€ä¸ªæ¶ˆæ¯ï¼Œè¡¨ç¤ºå‡½æ•°å·²æ‰§è¡Œ
    return result

# ç”¨äºå…³é—­æœåŠ¡çš„è·¯ç”±
@app.post("/shutdown")
async def shutdown(request: Request):


    # è·å–Uvicornçš„å®ä¾‹
    server = request.app.state.server
    # å…³é—­æœåŠ¡å™¨
    server.should_exit = True
    # è¿”å›ä¸€ä¸ªå“åº”ï¼Œè¡¨ç¤ºæœåŠ¡å™¨æ­£åœ¨å…³é—­
    return {"message": "Shutting down server."}

# ä¸»å‡½æ•°ï¼Œç”¨äºå¯åŠ¨UvicornæœåŠ¡å™¨
if __name__ == "__main__":
    # å°†UvicornæœåŠ¡å™¨å®ä¾‹å­˜å‚¨åœ¨åº”ç”¨ç¨‹åºçŠ¶æ€ä¸­
    server = uvicorn.Server(uvicorn.Config(app, host="0.0.0.0", port=1111))
    app.state.server = server
    # å¯åŠ¨æœåŠ¡å™¨
    server.run()

