# coding=utf-8

import os
import librosa
import base64
import io
import gradio as gr
import re
from fastapi import FastAPI, File, UploadFile
import numpy as np
import torch
import torchaudio

from pydub import AudioSegment
from pydub.silence import detect_nonsilent, split_on_silence



from funasr import AutoModel

model = "iic/SenseVoiceSmall"
model = AutoModel(model=model,
				  vad_model="iic/speech_fsmn_vad_zh-cn-16k-common-pytorch",
				  vad_kwargs={"max_single_segment_time": 30000},
				  trust_remote_code=True,
				  )

import re

emo_dict = {
	"<|HAPPY|>": "ğŸ˜Š",
	"<|SAD|>": "ğŸ˜”",
	"<|ANGRY|>": "ğŸ˜¡",
	"<|NEUTRAL|>": "",
	"<|FEARFUL|>": "ğŸ˜°",
	"<|DISGUSTED|>": "ğŸ¤¢",
	"<|SURPRISED|>": "ğŸ˜®",
}

event_dict = {
	"<|BGM|>": "ğŸ¼",
	"<|Speech|>": "",
	"<|Applause|>": "ğŸ‘",
	"<|Laughter|>": "ğŸ˜€",
	"<|Cry|>": "ğŸ˜­",
	"<|Sneeze|>": "ğŸ¤§",
	"<|Breath|>": "",
	"<|Cough|>": "ğŸ¤§",
}

emoji_dict = {
	"<|nospeech|><|Event_UNK|>": "â“",
	"<|zh|>": "",
	"<|en|>": "",
	"<|yue|>": "",
	"<|ja|>": "",
	"<|ko|>": "",
	"<|nospeech|>": "",
	"<|HAPPY|>": "ğŸ˜Š",
	"<|SAD|>": "ğŸ˜”",
	"<|ANGRY|>": "ğŸ˜¡",
	"<|NEUTRAL|>": "",
	"<|BGM|>": "ğŸ¼",
	"<|Speech|>": "",
	"<|Applause|>": "ğŸ‘",
	"<|Laughter|>": "ğŸ˜€",
	"<|FEARFUL|>": "ğŸ˜°",
	"<|DISGUSTED|>": "ğŸ¤¢",
	"<|SURPRISED|>": "ğŸ˜®",
	"<|Cry|>": "ğŸ˜­",
	"<|EMO_UNKNOWN|>": "",
	"<|Sneeze|>": "ğŸ¤§",
	"<|Breath|>": "",
	"<|Cough|>": "ğŸ˜·",
	"<|Sing|>": "",
	"<|Speech_Noise|>": "",
	"<|withitn|>": "",
	"<|woitn|>": "",
	"<|GBG|>": "",
	"<|Event_UNK|>": "",
}

lang_dict =  {
    "<|zh|>": "<|lang|>",
    "<|en|>": "<|lang|>",
    "<|yue|>": "<|lang|>",
    "<|ja|>": "<|lang|>",
    "<|ko|>": "<|lang|>",
    "<|nospeech|>": "<|lang|>",
}

emo_set = {"ğŸ˜Š", "ğŸ˜”", "ğŸ˜¡", "ğŸ˜°", "ğŸ¤¢", "ğŸ˜®"}
event_set = {"ğŸ¼", "ğŸ‘", "ğŸ˜€", "ğŸ˜­", "ğŸ¤§", "ğŸ˜·",}

def format_str(s):
	for sptk in emoji_dict:
		s = s.replace(sptk, emoji_dict[sptk])
	return s


def format_str_v2(s):
	sptk_dict = {}
	for sptk in emoji_dict:
		sptk_dict[sptk] = s.count(sptk)
		s = s.replace(sptk, "")
	emo = "<|NEUTRAL|>"
	for e in emo_dict:
		if sptk_dict[e] > sptk_dict[emo]:
			emo = e
	for e in event_dict:
		if sptk_dict[e] > 0:
			s = event_dict[e] + s
	s = s + emo_dict[emo]

	for emoji in emo_set.union(event_set):
		s = s.replace(" " + emoji, emoji)
		s = s.replace(emoji + " ", emoji)
	return s.strip()

def format_str_v3(s):
	def get_emo(s):
		return s[-1] if s[-1] in emo_set else None
	def get_event(s):
		return s[0] if s[0] in event_set else None

	s = s.replace("<|nospeech|><|Event_UNK|>", "â“")
	for lang in lang_dict:
		s = s.replace(lang, "<|lang|>")
	s_list = [format_str_v2(s_i).strip(" ") for s_i in s.split("<|lang|>")]
	new_s = " " + s_list[0]
	cur_ent_event = get_event(new_s)
	for i in range(1, len(s_list)):
		if len(s_list[i]) == 0:
			continue
		if get_event(s_list[i]) == cur_ent_event and get_event(s_list[i]) != None:
			s_list[i] = s_list[i][1:]
		#else:
		cur_ent_event = get_event(s_list[i])
		if get_emo(s_list[i]) != None and get_emo(s_list[i]) == get_emo(new_s):
			new_s = new_s[:-1]
		new_s += s_list[i].strip().lstrip()
	new_s = new_s.replace("The.", " ")
	return new_s.strip()

def model_inference(input_wav, language, fs=16000):
	# task_abbr = {"Speech Recognition": "ASR", "Rich Text Transcription": ("ASR", "AED", "SER")}
	language_abbr = {"auto": "auto", "zh": "zh", "en": "en", "yue": "yue", "ja": "ja", "ko": "ko",
					 "nospeech": "nospeech"}
	
	# task = "Speech Recognition" if task is None else task
	language = "auto" if len(language) < 1 else language
	selected_language = language_abbr[language]
	# selected_task = task_abbr.get(task)
	
	# print(f"input_wav: {type(input_wav)}, {input_wav[1].shape}, {input_wav}")
	
	if isinstance(input_wav, tuple):
		fs, input_wav = input_wav
		input_wav = input_wav.astype(np.float32) / np.iinfo(np.int16).max
		if len(input_wav.shape) > 1:
			input_wav = input_wav.mean(-1)
		if fs != 16000:
			print(f"audio_fs: {fs}")
			resampler = torchaudio.transforms.Resample(fs, 16000)
			input_wav_t = torch.from_numpy(input_wav).to(torch.float32)
			input_wav = resampler(input_wav_t[None, :])[0, :].numpy()
	
	
	merge_vad = True #False if selected_task == "ASR" else True
	print(f"language: {language}, merge_vad: {merge_vad}")
	text = model.generate(input=input_wav,
						  cache={},
						  language=language,
						  use_itn=True,
						  batch_size_s=0, merge_vad=merge_vad)
	
	print(text)
	text = text[0]["text"]
	text = format_str_v3(text)
	
	print(text)
	
	return text


def model_inference(input_wav, language, fs=16000):
    language_abbr = {"auto": "auto", "zh": "zh", "en": "en", "yue": "yue", "ja": "ja", "ko": "ko",
                     "nospeech": "nospeech"}
    
    language = "auto" if len(language) < 1 else language
    selected_language = language_abbr[language]
    
    if isinstance(input_wav, tuple):
        fs, input_wav = input_wav
        input_wav = input_wav.astype(np.float32) / np.iinfo(np.int16).max
        if len(input_wav.shape) > 1:
            input_wav = input_wav.mean(-1)
        if fs != 16000:
            print(f"audio_fs: {fs}")
            resampler = torchaudio.transforms.Resample(fs, 16000)
            input_wav_t = torch.from_numpy(input_wav).to(torch.float32)
            input_wav = resampler(input_wav_t[None, :])[0, :].numpy()
    
    merge_vad = True
    print(f"language: {language}, merge_vad: {merge_vad}")
    text = model.generate(input=input_wav,
                          cache={},
                          language=language,
                          use_itn=True,
                          batch_size_s=0, merge_vad=merge_vad)
    
    print(text)
    text = text[0]["text"]
    text = format_str_v3(text)
    
    print(text)
    
    return text
# è½¬æ¢æ¯«ç§’åˆ°SRTæ—¶é—´æ ¼å¼
def ms_to_srt_time(ms):
    seconds, milliseconds = divmod(ms, 1000)
    minutes, seconds = divmod(seconds, 60)
    hours, minutes = divmod(minutes, 60)
    return f"{int(hours):02}:{int(minutes):02}:{int(seconds):02},{int(milliseconds):03}"

def segment_and_transcribe(input_wav, language, output_path = "outsrt/transcription.srt"):

    if isinstance(input_wav, tuple):
        fs, input_wav = input_wav
        input_wav = input_wav.astype(np.float32) / np.iinfo(np.int16).max
        if len(input_wav.shape) > 1:
            input_wav = input_wav.mean(-1)
        if fs != 16000:
            resampler = torchaudio.transforms.Resample(fs, 16000)
            input_wav_t = torch.from_numpy(input_wav).to(torch.float32)
            input_wav = resampler(input_wav_t[None, :])[0, :].numpy()
        input_wav = (input_wav * np.iinfo(np.int16).max).astype(np.int16)

    audio_segment = AudioSegment(
        input_wav.tobytes(), 
        frame_rate=16000, 
        sample_width=input_wav.dtype.itemsize, 
        channels=1
    )

    nonsilent_ranges = detect_nonsilent(audio_segment, min_silence_len=500, silence_thresh=-40)
    srt_content = []
    start_time = 0
    counter = 1

    for start, end in nonsilent_ranges:
        # å¤„ç†é™éŸ³ç‰‡æ®µ
        if start_time < start:
            srt_content.append(f"{counter}\n{ms_to_srt_time(start_time)} --> {ms_to_srt_time(start)}\n\n")
            counter += 1
        # å¤„ç†éé™éŸ³ç‰‡æ®µ
        chunk = audio_segment[start:end]
        segment_wav = np.array(chunk.get_array_of_samples())
        segment_wav = (segment_wav / np.iinfo(np.int16).max).astype(np.float32)
        
        transcription = model_inference(segment_wav, language)
        
        srt_content.append(f"{counter}\n{ms_to_srt_time(start)} --> {ms_to_srt_time(end)}\n{transcription}\n")
        counter += 1
        start_time = end  # æ›´æ–°ä¸‹ä¸€ä¸ªç‰‡æ®µçš„å¼€å§‹æ—¶é—´
    
    # å¤„ç†éŸ³é¢‘æœ«å°¾çš„é™éŸ³æ®µ
    if start_time < len(audio_segment):
        srt_content.append(f"{counter}\n{ms_to_srt_time(start_time)} --> {ms_to_srt_time(len(audio_segment))}\n\n")

    srt_text = "\n".join(srt_content)
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(srt_text)
    
    print(f"SRT file saved to {output_path}")
    return srt_text


    

html_content = """
<div>
    <h2 style="font-size: 22px;margin-left: 0px;">å£°éŸ³ç†è§£æ¨¡å‹ï¼šSenseVoice-Small</h2>
    <p style="font-size: 18px;margin-left: 20px;">SenseVoice-Small æ˜¯ä¸€ä¸ªç¼–ç å™¨åŸºç¡€çš„å£°éŸ³åŸºç¡€æ¨¡å‹ï¼Œä¸“ä¸ºå¿«é€Ÿå£°éŸ³ç†è§£è€Œè®¾è®¡ã€‚å®ƒåŒ…å«äº†å¤šç§ç‰¹æ€§ï¼ŒåŒ…æ‹¬è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€å£è¯­è¯†åˆ«ï¼ˆLIDï¼‰ã€è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰å’Œå£°å­¦äº‹ä»¶æ£€æµ‹ï¼ˆAEDï¼‰ã€‚SenseVoice-Small æ”¯æŒä¸­æ–‡ã€è‹±æ–‡ã€ç²¤è¯­ã€æ—¥è¯­å’ŒéŸ©è¯­çš„å¤šè¯­ç§è¯†åˆ«ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜æä¾›äº†å¼‚å¸¸ä½çš„æ¨ç†å»¶è¿Ÿï¼Œæ¯” Whisper-small å¿« 7 å€ï¼Œæ¯” Whisper-large å¿« 17 å€ã€‚</p>
    <h2 style="font-size: 22px;margin-left: 0px;">ä½¿ç”¨æ–¹æ³•</h2> 
    <p style="font-size: 18px;margin-left: 20px;">ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶æˆ–é€šè¿‡éº¦å…‹é£è¾“å…¥ï¼Œç„¶åé€‰æ‹©ä»»åŠ¡å’Œè¯­è¨€ã€‚éŸ³é¢‘å°†è¢«è½¬å½•æˆç›¸åº”çš„æ–‡æœ¬ï¼Œå¹¶é™„å¸¦ç›¸å…³çš„æƒ…æ„Ÿï¼ˆğŸ˜Š å¿«ä¹ï¼ŒğŸ˜¡ ç”Ÿæ°”/å…´å¥‹ï¼ŒğŸ˜” æ‚²ä¼¤ï¼‰å’Œå£°éŸ³äº‹ä»¶ç±»å‹ï¼ˆğŸ˜€ ç¬‘å£°ï¼ŒğŸ¼ éŸ³ä¹ï¼ŒğŸ‘ æŒå£°ï¼ŒğŸ¤§ å’³å—½å’Œæ‰“å–·åšï¼ŒğŸ˜­ å“­æ³£ï¼‰ã€‚äº‹ä»¶æ ‡ç­¾å°†æ”¾ç½®åœ¨æ–‡æœ¬çš„å‰é¢ï¼Œæƒ…æ„Ÿæ ‡ç­¾å°†æ”¾åœ¨æ–‡æœ¬çš„åé¢ã€‚</p>
    <p style="font-size: 18px;margin-left: 20px;">æ¨èéŸ³é¢‘è¾“å…¥æ—¶é•¿ä¸è¶…è¿‡ 30 ç§’ã€‚å¯¹äºè¶…è¿‡ 30 ç§’çš„éŸ³é¢‘ï¼Œå»ºè®®æœ¬åœ°éƒ¨ç½²ã€‚</p>
    <h2 style="font-size: 22px;margin-left: 0px;">ä»£ç ä»“åº“</h2>
    <p style="font-size: 18px;margin-left: 20px;"><a href="https://github.com/FunAudioLLM/SenseVoice" target="_blank">SenseVoice</a>: å¤šè¯­ç§è¯­éŸ³ç†è§£æ¨¡å‹</p>
    <p style="font-size: 18px;margin-left: 20px;"><a href="https://github.com/modelscope/FunASR" target="_blank">FunASR</a>: åŸºç¡€è¯­éŸ³è¯†åˆ«å·¥å…·åŒ…</p>
    <p style="font-size: 18px;margin-left: 20px;"><a href="https://github.com/FunAudioLLM/CosyVoice" target="_blank">CosyVoice</a>: é«˜å“è´¨å¤šè¯­ç§TTSæ¨¡å‹</p>
</div>
"""


def launch():
	with gr.Blocks(theme=gr.themes.Soft()) as demo:
		# gr.Markdown(description)
		gr.HTML(html_content)
		with gr.Row():
			with gr.Column():
				audio_inputs = gr.Audio(label="ä¸Šä¼ éŸ³é¢‘")
				with gr.Accordion("Configuration"):
					# task_inputs = gr.Radio(choices=["Speech Recognition", "Rich Text Transcription"],
					# 					   value="Speech Recognition", label="Task")
					language_inputs = gr.Dropdown(choices=["auto", "zh", "en", "yue", "ja", "ko", "nospeech"],
												  value="auto",
												  label="Language")
			with gr.Column():		
				fn_button = gr.Button("ä¸€èˆ¬è¾“å‡º", variant="primary")
				fn_button1 = gr.Button("å­—å¹•è¾“å‡º", variant="primary")				
				text_outputs = gr.Textbox(label="è¾“å‡ºç»“æœ")
		
		fn_button.click(model_inference, inputs=[audio_inputs, language_inputs], outputs=text_outputs)
		fn_button1.click(segment_and_transcribe, inputs=[audio_inputs, language_inputs], outputs=text_outputs)
		# with gr.Accordion("More examples"):
		# 	gr.HTML(centered_table_html)
	demo.launch()


if __name__ == "__main__":
	# iface.launch()
	launch()


